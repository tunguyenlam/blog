{
  
    
        "post0": {
            "title": "Is cosine similarity good enough",
            "content": "cosine similarity . cosine similarity is the most popular approach to assess similarity between datapoints in Machine Learning. dot(A,B)=∣A∣∣B∣cos(A,B)dot(A,B) = |A||B|cos(A,B)dot(A,B)=∣A∣∣B∣cos(A,B) . The weakness of Cosine Similarity . However, Cosine Similarity cannot measure the attitude of datapoints in space. .",
            "url": "https://tunguyenlam.github.io/blog/2024/03/24/Is-cosine-similarity-good-enough.html",
            "relUrl": "/2024/03/24/Is-cosine-similarity-good-enough.html",
            "date": " • Mar 24, 2024"
        }
        
    
  
    
        ,"post1": {
            "title": "How to encode categorical feature",
            "content": "Abstraction . Encoding is a way we transform data from a representation to another representation. In Machine Learning, we usually use this concept when we want to transform non-numeric data to numeric data. such as, we transform label &quot;cat&quot;, &quot;dog&quot; to 1, 0 respectively. . There are some popular ways to encode categorical feature: Using dictionary to convert data to index, using one-hot vector approach, or we just train a neural network and encode data as word2vec. . Problem of some popular encoding techniques . 1) one-hot: memory . 2) Using dictionary:big dictionary, unknown item . 3) word2vec: unknown item . 4) neural network: need to represent input in some way first, then we need to be back to 1) 2)3). Sometimes, we will get too complex output. eg: encode by GPT. . Addtionally, there are some cases we use sparse matrix to encode.... . import pandas as pd # Sample dataset with a categorical column data = {&#39;Color&#39;: [&#39;Red&#39;, &#39;Blue&#39;, &#39;Green&#39;, &#39;Red&#39;, &#39;Green&#39;]} df = pd.DataFrame(data) # Perform one-hot encoding using Pandas one_hot_encoded = pd.get_dummies(df, columns=[&#39;Color&#39;]).astype(int) one_hot_encoded . Color_Blue Color_Green Color_Red . 0 0 | 0 | 1 | . 1 1 | 0 | 0 | . 2 0 | 1 | 0 | . 3 0 | 0 | 1 | . 4 0 | 1 | 0 | . from sklearn.preprocessing import LabelEncoder data = {&#39;Size&#39;: [&#39;Small&#39;, &#39;Medium&#39;, &#39;Large&#39;, &#39;Medium&#39;, &#39;Small&#39;]} df = pd.DataFrame(data) label_encoder = LabelEncoder() df[&#39;Size_encoded&#39;] = label_encoder.fit_transform(df[&#39;Size&#39;]) df . Size Size_encoded . 0 Small | 2 | . 1 Medium | 1 | . 2 Large | 0 | . 3 Medium | 1 | . 4 Small | 2 | . from numpy import array from scipy.sparse import csr_matrix A = array([[1, 0, 0, 1, 0, 0], [0, 0, 2, 0, 0, 1], [0, 0, 0, 2, 0, 0]]) print(A) print(&quot;&quot;) S = csr_matrix(A) print(S) . [[1 0 0 1 0 0] [0 0 2 0 0 1] [0 0 0 2 0 0]] (0, 0) 1 (0, 3) 1 (1, 2) 2 (1, 5) 1 (2, 3) 2 . Solution: Byte Pair Encoding . Byte Pair Encoding . Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is &quot;byte-level&quot; because it runs on UTF-8 encoded strings. . The tricky thing to note is that minbpe always allocates the 256 individual bytes as tokens, and then merges bytes as needed from there. So for us a=97, b=98, c=99, d=100 (their ASCII values). . References: https://en.wikipedia.org/wiki/Byte_pair_encoding . https://github.com/karpathy/minbpe . import sys from utils import * auto_config() from minbpe.minbpe import BasicTokenizer tokenizer = BasicTokenizer() text = &quot;aaabdaaabac&quot; tokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges print(tokenizer.encode(text)) # [258, 100, 258, 97, 99] print(tokenizer.decode([258, 100, 258, 97, 99])) . [258, 100, 258, 97, 99] aaabdaaabac . unknown_word = &quot;hehe&quot; tokenizer.encode(unknown_word) . [104, 101, 104, 101] .",
            "url": "https://tunguyenlam.github.io/blog/2024/02/29/How-to-encode-categorical-feature.html",
            "relUrl": "/2024/02/29/How-to-encode-categorical-feature.html",
            "date": " • Feb 29, 2024"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is blog where I’m able to write everything about Machine Learning. .",
          "url": "https://tunguyenlam.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About",
          "content": "About this site .",
          "url": "https://tunguyenlam.github.io/blog/about.qmd",
          "relUrl": "/about.qmd",
          "date": ""
      }
      
  

  

  
      ,"page4": {
          "title": "blog",
          "content": "This is a Quarto website. . To learn more about Quarto websites visit https://quarto.org/docs/websites. .",
          "url": "https://tunguyenlam.github.io/blog/index.qmd",
          "relUrl": "/index.qmd",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tunguyenlam.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}