{
  
    
        "post0": {
            "title": "Sample of Market Volatility",
            "content": "Data generation . yfinance offers a threaded and Pythonic way to download market data from Yahoo!Ⓡ finance. . import numpy as np import pandas as pd import yfinance as yf # Download historical data for a few example assets assets = [&#39;BTC-USD&#39;, &#39;ETH-USD&#39;, &#39;USDT-USD&#39;] data = yf.download(assets, start=&quot;2020-01-01&quot;, end=&quot;2023-01-01&quot;)[&#39;Adj Close&#39;] returns = data.pct_change().dropna() market_returns = yf.download(&#39;^GSPC&#39;, start=&quot;2020-01-01&quot;, end=&quot;2023-01-01&quot;)[&#39;Adj Close&#39;].pct_change().dropna() data.head() . [*********************100%%**********************] 3 of 3 completed [*********************100%%**********************] 1 of 1 completed . Ticker BTC-USD ETH-USD USDT-USD . Date . 2020-01-01 7200.174316 | 130.802002 | 0.999836 | . 2020-01-02 6985.470215 | 127.410179 | 1.001565 | . 2020-01-03 7344.884277 | 134.171707 | 1.004192 | . 2020-01-04 7410.656738 | 135.069366 | 1.007472 | . 2020-01-05 7411.317383 | 136.276779 | 1.006197 | . Volatility Calculation . volatility = returns.std() # Annualized Volatility annualized_volatility = volatility * np.sqrt(252) annualized_volatility . Ticker BTC-USD 0.602380 ETH-USD 0.801935 USDT-USD 0.052029 dtype: float64 . Historical Drawdown . def max_drawdown(returns): cumulative = (1 + returns).cumprod() peak = cumulative.cummax() drawdown = (cumulative - peak) / peak return drawdown.min() drawdowns = returns.apply(max_drawdown) drawdowns . Ticker BTC-USD -0.766346 ETH-USD -0.793512 USDT-USD -0.075302 dtype: float64 . Beta Coefficient . def calculate_beta(returns, market_returns): covariance = returns.cov(market_returns) market_variance = market_returns.var() beta = covariance / market_variance return beta betas = returns.apply(lambda x: calculate_beta(x, market_returns)) betas . Ticker BTC-USD 1.068543 ETH-USD 1.373727 USDT-USD -0.051275 dtype: float64 . Correlation Analysis . correlation_matrix = returns.corr() correlation_matrix . Ticker BTC-USD ETH-USD USDT-USD . Ticker . BTC-USD 1.000000 | 0.822365 | -0.168333 | . ETH-USD 0.822365 | 1.000000 | -0.171933 | . USDT-USD -0.168333 | -0.171933 | 1.000000 | . Complexity-Entropy Causality Plane . AntroPy is a Python 3 package providing several time-efficient algorithms for computing the complexity of time-series. . import antropy as ant def permutation_entropy(time_series, emb_dim): return ant.perm_entropy(time_series, order=emb_dim) permutation_entropies = returns.apply(lambda x: permutation_entropy(x.values, 3)) permutation_entropies . Ticker BTC-USD 2.582325 ETH-USD 2.582585 USDT-USD 2.577085 dtype: float64 . Application . risk_levels = pd.DataFrame({ &#39;Volatility&#39;: annualized_volatility, &#39;Drawdown&#39;: drawdowns, &#39;Beta&#39;: betas, &#39;Complexity&#39;: permutation_entropies }) risk_levels[&#39;RiskCategory&#39;] = &#39;Low&#39; risk_levels.loc[(risk_levels[&#39;Volatility&#39;] &gt; risk_levels[&#39;Volatility&#39;].quantile(0.33)) &amp; (risk_levels[&#39;Volatility&#39;] &lt;= risk_levels[&#39;Volatility&#39;].quantile(0.66)) &amp; (risk_levels[&#39;Drawdown&#39;] &gt;= risk_levels[&#39;Drawdown&#39;].quantile(0.33)) &amp; (risk_levels[&#39;Drawdown&#39;] &lt;= risk_levels[&#39;Drawdown&#39;].quantile(0.66)) &amp; (risk_levels[&#39;Beta&#39;] &lt;= 1) &amp; (risk_levels[&#39;Complexity&#39;] &lt;= risk_levels[&#39;Complexity&#39;].quantile(0.66)), &#39;RiskCategory&#39;] = &#39;Medium&#39; risk_levels.loc[(risk_levels[&#39;Volatility&#39;] &gt; risk_levels[&#39;Volatility&#39;].quantile(0.66)) | (risk_levels[&#39;Drawdown&#39;] &lt; risk_levels[&#39;Drawdown&#39;].quantile(0.33)) | (risk_levels[&#39;Beta&#39;] &gt; 1) | (risk_levels[&#39;Complexity&#39;] &gt; risk_levels[&#39;Complexity&#39;].quantile(0.66)), &#39;RiskCategory&#39;] = &#39;High&#39; risk_levels . Volatility Drawdown Beta Complexity RiskCategory . Ticker . BTC-USD 0.602380 | -0.766346 | 1.068543 | 2.582325 | High | . ETH-USD 0.801935 | -0.793512 | 1.373727 | 2.582585 | High | . USDT-USD 0.052029 | -0.075302 | -0.051275 | 2.577085 | Low | . import matplotlib.pyplot as plt plt.scatter(risk_levels[&#39;Volatility&#39;], risk_levels[&#39;Drawdown&#39;], c=risk_levels[&#39;RiskCategory&#39;].map({&#39;Low&#39;: &#39;green&#39;, &#39;Medium&#39;: &#39;yellow&#39;, &#39;High&#39;: &#39;red&#39;})) plt.xlabel(&#39;Annualized Volatility&#39;) plt.ylabel(&#39;Drawdown&#39;) plt.title(&#39;Liquidity Pool Risk Categorization&#39;) plt.show() .",
            "url": "https://tunguyenlam.github.io/blog/2024/05/30/Market-Volatility.html",
            "relUrl": "/2024/05/30/Market-Volatility.html",
            "date": " • May 30, 2024"
        }
        
    
  
    
        ,"post1": {
            "title": "Sample of liquidity risk assessment",
            "content": "We generate a simple demo data as below: . import pandas as pd df = pd.read_csv(&quot;sample_data_of_liquidity_risk_assessment.csv&quot;) df . Cryptocurrency Trading Volume (M) Transaction Count Bid-Ask Spread (%) Market Depth Historical Liquidity Exchange Presence . 0 BTC | 50 | 200000 | 0.1 | High | Very Liquid | Major Exchanges | . 1 ETH | 30 | 150000 | 0.2 | High | Mostly Liquid | Major Exchanges | . 2 XRP | 15 | 100000 | 0.5 | Moderate | Moderately Liquid | Major Exchanges | . 3 LTC | 10 | 80000 | 0.8 | Low | Affected | Major Exchanges | . Compute risk . $Liquidity Risk Score= Trading Volume Score+Bid-Ask Spread Score+Market Depth Score+Historical Liquidity Score+Market Presence Score$ ​ . cryptos = { &quot;BTC&quot;: {&quot;Trading Volume&quot;: 1, &quot;Bid-Ask Spread&quot;: 1, &quot;Market Depth&quot;: 2, &quot;Historical Liquidity&quot;: 1, &quot;Market Presence&quot;: 1}, &quot;ETH&quot;: {&quot;Trading Volume&quot;: 2, &quot;Bid-Ask Spread&quot;: 2, &quot;Market Depth&quot;: 2, &quot;Historical Liquidity&quot;: 2, &quot;Market Presence&quot;: 1}, &quot;XRP&quot;: {&quot;Trading Volume&quot;: 3, &quot;Bid-Ask Spread&quot;: 3, &quot;Market Depth&quot;: 3, &quot;Historical Liquidity&quot;: 3, &quot;Market Presence&quot;: 1}, &quot;LTC&quot;: {&quot;Trading Volume&quot;: 3, &quot;Bid-Ask Spread&quot;: 3, &quot;Market Depth&quot;: 4, &quot;Historical Liquidity&quot;: 4, &quot;Market Presence&quot;: 1} } # Calculate the liquidity risk score def calculate_liquidity_risk_score(data): scores = {} for crypto, factors in data.items(): score = sum(factors.values()) / len(factors) scores[crypto] = score return scores # Calculate scores liquidity_risk_scores = calculate_liquidity_risk_score(cryptos) liquidity_risk_scores . {&#39;BTC&#39;: 1.2, &#39;ETH&#39;: 1.8, &#39;XRP&#39;: 2.6, &#39;LTC&#39;: 3.0} . Low Liquidity Risk: BTC, ETH | Moderate Liquidity Risk: XRP, LTC | .",
            "url": "https://tunguyenlam.github.io/blog/2024/05/29/Liquidity_risk_assessment.html",
            "relUrl": "/2024/05/29/Liquidity_risk_assessment.html",
            "date": " • May 29, 2024"
        }
        
    
  
    
        ,"post2": {
            "title": "Pool Liquidity Sampling",
            "content": "Example of Pool Liquidity . By defining a probabilistic model, estimate the distribution of an asset in the liquidity pool and identify the equilibrium level that ensures sufficient liquidity without impacting the market price. . Data Generation Generate data on inflows, outflows, and transaction volumes. | . | Model Parameter Estimation Estimate $ lambda_I, lambda_O, alpha$ and $ beta$ using historical data. | . | Simulation Perform Monte Carlo simulations to generate random samples of inflows, outflows, and transaction volumes. | Calculate liquidity supply and demand for each simulation step. | . | Equilibrium Analysis Determine the distribution of liquidity levels and identify the equilibrium range. | Calculate confidence intervals to quantify uncertainty. | . | Visualization Plot the probability distribution of liquidity levels. | Highlight the equilibrium range and confidence intervals. | . | import numpy as np import matplotlib.pyplot as plt import jax.numpy as jnp import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC, NUTS, Predictive from jax import random . historical_inflows = np.random.poisson(lam=10, size=100) historical_outflows = np.random.poisson(lam=8, size=100) historical_volumes = np.random.gamma(shape=2.0, scale=2.0, size=100) . def model(inflows=None, outflows=None, volumes=None): lambda_I = numpyro.sample(&#39;lambda_I&#39;, dist.Exponential(1.0)) lambda_O = numpyro.sample(&#39;lambda_O&#39;, dist.Exponential(1.0)) alpha = numpyro.sample(&#39;alpha&#39;, dist.Gamma(2.0, 1.0)) beta = numpyro.sample(&#39;beta&#39;, dist.Gamma(2.0, 1.0)) with numpyro.plate(&#39;data&#39;, len(inflows) if inflows is not None else 1): if inflows is not None: numpyro.sample(&#39;obs_inflows&#39;, dist.Poisson(lambda_I), obs=inflows) if outflows is not None: numpyro.sample(&#39;obs_outflows&#39;, dist.Poisson(lambda_O), obs=outflows) if volumes is not None: numpyro.sample(&#39;obs_volumes&#39;, dist.Gamma(alpha, beta), obs=volumes) nuts_kernel = NUTS(model) mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=2000) mcmc.run(random.PRNGKey(0), inflows=historical_inflows, outflows=historical_outflows, volumes=historical_volumes) samples = mcmc.get_samples() . sample: 100%|██████████| 2500/2500 [00:01&lt;00:00, 1810.22it/s, 7 steps of size 3.99e-01. acc. prob=0.93] . num_simulations = 1000 # Use the estimated parameters to generate new samples def simulate(): lambda_I = numpyro.sample(&#39;lambda_I&#39;, dist.Exponential(1.0)) lambda_O = numpyro.sample(&#39;lambda_O&#39;, dist.Exponential(1.0)) alpha = numpyro.sample(&#39;alpha&#39;, dist.Gamma(2.0, 1.0)) beta = numpyro.sample(&#39;beta&#39;, dist.Gamma(2.0, 1.0)) inflows = numpyro.sample(&#39;inflows&#39;, dist.Poisson(lambda_I)) outflows = numpyro.sample(&#39;outflows&#39;, dist.Poisson(lambda_O)) volumes = numpyro.sample(&#39;volumes&#39;, dist.Gamma(alpha, beta)) return inflows, outflows, volumes predictive = Predictive(simulate, posterior_samples=samples, num_samples=num_simulations) simulated_data = predictive(random.PRNGKey(1)) simulated_inflows = simulated_data[&#39;inflows&#39;] simulated_outflows = simulated_data[&#39;outflows&#39;] simulated_volumes = simulated_data[&#39;volumes&#39;] # Calculate liquidity supply and demand for each simulation step liquidity_supply = simulated_inflows liquidity_demand = simulated_outflows . /var/folders/04/kxzm_nrn6w9fwt4f_qnyr1m00000gn/T/ipykernel_5796/3789121203.py:17: UserWarning: Sample&#39;s batch dimension size 2000 is different from the provided 1000 num_samples argument. Defaulting to 2000. predictive = Predictive(simulate, posterior_samples=samples, num_samples=num_simulations) . import numpy as np import scipy.stats def standard_deviation_rule(data, level=95): percentiles = { 68: [16, 84], 95: [2.5, 97.5], 99.7:[0.15, 99.85] } percentiles = percentiles[level] output_range = (np.percentile(data, percentiles[0]), np.percentile(data, percentiles[1])) return output_range equilibrium_levels = liquidity_supply - liquidity_demand yellow, green, purple = np.array([68, 95, 99.7]) yellow_range = standard_deviation_rule(equilibrium_levels, level = yellow) green_range = standard_deviation_rule(equilibrium_levels, level = green) purple_range = standard_deviation_rule(equilibrium_levels, level = purple) . plt.figure(figsize=(10, 6)) plt.hist(equilibrium_levels, bins=30, density=True, alpha=0.6, color=&#39;g&#39;, label=&#39;Equilibrium Levels&#39;) plt.axvline(yellow_range[0], color=&#39;y&#39;, linestyle=&#39;dashed&#39;, linewidth=1, label=&#39;yellow level&#39;) plt.axvline(yellow_range[1], color=&#39;y&#39;, linestyle=&#39;dashed&#39;, linewidth=1, label=&#39;yellow level&#39;) plt.axvline(green_range[0], color=&#39;g&#39;, linestyle=&#39;dashed&#39;, linewidth=1, label=&#39;green level&#39;) plt.axvline(green_range[1], color=&#39;g&#39;, linestyle=&#39;dashed&#39;, linewidth=1, label=&#39;green level&#39;) plt.axvline(purple_range[0], color=&#39;purple&#39;, linestyle=&#39;dashed&#39;, linewidth=1, label=&#39;purple level&#39;) plt.axvline(purple_range[1], color=&#39;purple&#39;, linestyle=&#39;dashed&#39;, linewidth=1, label=&#39;purple level&#39;) plt.xlabel(&#39;Equilibrium Levels&#39;) plt.ylabel(&#39;Density&#39;) plt.title(&#39;Probability Distribution of Equilibrium Levels&#39;) plt.legend() plt.show() .",
            "url": "https://tunguyenlam.github.io/blog/2024/05/28/Forest_Finance_Pool_Liquidity.html",
            "relUrl": "/2024/05/28/Forest_Finance_Pool_Liquidity.html",
            "date": " • May 28, 2024"
        }
        
    
  
    
        ,"post3": {
            "title": "Transform user into vector space",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . Data . We&#39;ll build a small program to generate a sample dataset of user that has schema below: . Wallet Address | Transaction Count | Transaction Volume | Avg Transaction Value | Transaction Frequency | Asset Diversity | Liquidity Provision | Staking Activity | Yield Farming Participation | Borrowing Value | Borrowing Frequency | Lending Value | Lending Frequency | Active Periods| Transaction Recency | Historical Activity Trends . import random import string def random_wallet_address(): return &quot;0x&quot; + &#39;&#39;.join(random.choices(string.ascii_letters + string.digits, k=40)) def random_time_frame(): return random.choice([&#39;Daily&#39;, &#39;Weekly&#39;, &#39;Bi-weekly&#39;, &#39;Monthly&#39;, &#39;Never&#39;]) def random_activity_trend(): return random.choice([&#39;Increasing&#39;, &#39;Stable&#39;, &#39;Decreasing&#39;]) def random_active_periods(): periods = [ &#39;Weekdays 10am-4pm&#39;, &#39;Weekdays 6pm-10pm&#39;, &#39;Weekdays 9am-5pm&#39;, &#39;Weekdays 7am-9am&#39;, &#39;Weekends 1pm-5pm&#39;, &#39;Weekends 8pm-12am&#39;, &#39;Weekends 6pm-10pm&#39;, &#39;Weekdays 11am-3pm&#39;, &#39;Weekends 10am-2pm&#39; ] return random.choice(periods) # Generate 100 records data = [] for _ in range(100): record = { &quot;Wallet Address&quot;: random_wallet_address(), &quot;Transaction Count&quot;: random.randint(10, 200), &quot;Transaction Volume&quot;: random.randint(1000, 50000), &quot;Avg Transaction Value&quot;: random.randint(200, 300), &quot;Transaction Frequency&quot;: random_time_frame(), &quot;Asset Diversity&quot;: random.randint(1, 10), &quot;Liquidity Provision&quot;: random.randint(1000, 20000), &quot;Staking Activity&quot;: random.randint(500, 10000), &quot;Yield Farming Participation&quot;: random.randint(0, 15000), &quot;Borrowing Value&quot;: random.randint(0, 5000), &quot;Borrowing Frequency&quot;: random_time_frame(), &quot;Lending Value&quot;: random.randint(0, 7000), &quot;Lending Frequency&quot;: random_time_frame(), &quot;Active Periods&quot;: random_active_periods(), &quot;Transaction Recency&quot;: random.choice([&quot;1 day ago&quot;, &quot;2 days ago&quot;, &quot;3 days ago&quot;, &quot;1 week ago&quot;, &quot;2 weeks ago&quot;, &quot;1 month ago&quot;]), &quot;Historical Activity Trends&quot;: random_activity_trend() } data.append(record) df = pd.DataFrame(data=data) df.head(10) . Wallet Address Transaction Count Transaction Volume Avg Transaction Value Transaction Frequency Asset Diversity Liquidity Provision Staking Activity Yield Farming Participation Borrowing Value Borrowing Frequency Lending Value Lending Frequency Active Periods Transaction Recency Historical Activity Trends . 0 0x4w5qFhl7wgFLwVE0ZlaHk6sjNWd6xliLmn8vLq97 | 154 | 18248 | 274 | Never | 5 | 5926 | 7232 | 11620 | 937 | Bi-weekly | 1065 | Never | Weekdays 11am-3pm | 2 weeks ago | Decreasing | . 1 0x2M6KO5TvGez2LCn4weFdI4RXBRJ3kTVEBWtIK0n0 | 23 | 16162 | 298 | Bi-weekly | 3 | 1110 | 6273 | 1872 | 1405 | Bi-weekly | 312 | Bi-weekly | Weekends 8pm-12am | 1 week ago | Stable | . 2 0xbpXxykLr15t73HAexiUlOHzfiwNuxfnCYiiUyFdd | 31 | 42378 | 252 | Bi-weekly | 5 | 12326 | 9085 | 3379 | 1913 | Monthly | 4942 | Never | Weekdays 10am-4pm | 1 day ago | Decreasing | . 3 0x2XGUehw6PlEYe69NZk1Qtf8yMjoylhz1heo1oDJi | 141 | 13233 | 300 | Never | 1 | 19476 | 8981 | 2560 | 2388 | Monthly | 200 | Weekly | Weekends 6pm-10pm | 1 month ago | Decreasing | . 4 0xs0QX4UfexwYd0quzW7P9Hw3uHTTTwG2jx9lkdoRg | 112 | 2435 | 296 | Monthly | 7 | 12749 | 5968 | 2967 | 4038 | Weekly | 6802 | Never | Weekends 10am-2pm | 1 day ago | Increasing | . 5 0xwtXhxzRgb8G1tD065JQkTgqi1HmwYmEnaZYVdzyq | 138 | 42199 | 218 | Bi-weekly | 7 | 11407 | 7306 | 9080 | 4033 | Bi-weekly | 1695 | Weekly | Weekdays 7am-9am | 1 week ago | Increasing | . 6 0xqSKdKE0FdOeLTiHI08WfGsYLJfRz4OMDZIbFMLmV | 54 | 10064 | 218 | Monthly | 5 | 16907 | 4248 | 12041 | 4489 | Never | 2352 | Bi-weekly | Weekends 8pm-12am | 1 month ago | Increasing | . 7 0xVDk4jvAWlDpaEkhMhrxZKr0p7uqpHsz7mee96Lv6 | 164 | 23546 | 246 | Weekly | 7 | 3104 | 1992 | 7173 | 2060 | Monthly | 930 | Never | Weekdays 9am-5pm | 2 days ago | Increasing | . 8 0xgTlUywOCKmtntLGJOQnbTn8YcJFDZPCvNIZtMqID | 162 | 43583 | 260 | Weekly | 8 | 11279 | 7559 | 3514 | 1115 | Bi-weekly | 2307 | Monthly | Weekdays 10am-4pm | 2 weeks ago | Increasing | . 9 0xUhg8trjjhVhBRrIblUT0HTX4dRdn2XmlTY3aAzX4 | 63 | 5062 | 264 | Weekly | 4 | 18988 | 4557 | 6130 | 488 | Weekly | 1183 | Monthly | Weekends 8pm-12am | 1 day ago | Increasing | . Transform user to vector . $$u_i = f(x_i)$$ . $f$:is a transform function. . $x_i$: is a features set of user. . $u_i$: is a vector of specific user. . from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder def transform_function(df): ordinal_features = [&quot;Transaction Frequency&quot;, &quot;Historical Activity Trends&quot;, &quot;Transaction Recency&quot;] ordinal_encoder = OrdinalEncoder(categories=[[&quot;Never&quot;, &quot;Monthly&quot;, &quot;Bi-weekly&quot;, &quot;Weekly&quot;, &quot;Daily&quot;], [&quot;Decreasing&quot;, &quot;Stable&quot;, &quot;Increasing&quot;], [&quot;1 day ago&quot;, &quot;2 days ago&quot;, &quot;3 days ago&quot;, &quot;1 week ago&quot;, &quot;2 weeks ago&quot;, &quot;1 month ago&quot;]]) df[ordinal_features] = ordinal_encoder.fit_transform(df[ordinal_features]) onehot_features = [&quot;Active Periods&quot;] onehot_encoder = OneHotEncoder() onehot_encoded = onehot_encoder.fit_transform(df[onehot_features]).toarray() onehot_encoded_df = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out(onehot_features)) df = pd.concat([df.drop(columns=[&quot;Wallet Address&quot;, &quot;Active Periods&quot;]), onehot_encoded_df], axis=1) return df df = transform_function(df) . pd.options.display.max_colwidth = None pd.options.display.max_columns = None df = df.drop(columns=[&#39;Lending Frequency&#39;, &#39;Borrowing Frequency&#39;]) df.head() . Transaction Count Transaction Volume Avg Transaction Value Transaction Frequency Asset Diversity Liquidity Provision Staking Activity Yield Farming Participation Borrowing Value Lending Value Transaction Recency Historical Activity Trends Active Periods_Weekdays 10am-4pm Active Periods_Weekdays 11am-3pm Active Periods_Weekdays 6pm-10pm Active Periods_Weekdays 7am-9am Active Periods_Weekdays 9am-5pm Active Periods_Weekends 10am-2pm Active Periods_Weekends 1pm-5pm Active Periods_Weekends 6pm-10pm Active Periods_Weekends 8pm-12am . 0 154 | 18248 | 274 | 0.0 | 5 | 5926 | 7232 | 11620 | 937 | 1065 | 4.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 23 | 16162 | 298 | 2.0 | 3 | 1110 | 6273 | 1872 | 1405 | 312 | 3.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 2 31 | 42378 | 252 | 2.0 | 5 | 12326 | 9085 | 3379 | 1913 | 4942 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 141 | 13233 | 300 | 0.0 | 1 | 19476 | 8981 | 2560 | 2388 | 200 | 5.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | . 4 112 | 2435 | 296 | 1.0 | 7 | 12749 | 5968 | 2967 | 4038 | 6802 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | . Similarity . Assume we have a set of users that represented by matrix $P$ where each user is presented by vector $p$. Now, we want to list all of top (eg: 10) user that they are the most similar with user $p_i$. . From matrix $P$, we compute covariance matrix $Q$, that is formed: $$ begin{aligned} Q = P cdot P^T end{aligned} $$ . The, the similar score of $p_i$ will be $q_i$, to get the top similarity of $p_i$: $$top_i = argmax(q_i)$$ . (we also ignore product $i$ in the top_i as itself similar score is 1) . user_matrix = df.astype(float).to_numpy() norm_products = user_matrix/np.linalg.norm(user_matrix, axis=1)[:, None] Q = np.dot(norm_products, norm_products.T) fig, ax = plt.subplots(figsize = (20, 20)) sns.heatmap(Q, annot=True, fmt=&#39;.2f&#39;, cmap=&#39;RdYlGn&#39;) plt.show() . sns.heatmap(Q[:10, :10], annot=True, fmt=&#39;.2f&#39;, cmap=&#39;RdYlGn&#39;) plt.show() . Classification users . Implement simple classification on this data shows the highest participation in staking... . import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import classification_report, accuracy_score from imblearn.over_sampling import SMOTE # Assuming the data has already been prepared and encoded as in the previous steps # Define high participation in staking as above the median threshold = df[&#39;Staking Activity&#39;].quantile(0.8) df[&#39;High Staking Participation&#39;] = (df[&#39;Staking Activity&#39;] &gt; threshold).astype(int) # Features and target X = df.drop(columns=[&#39;Staking Activity&#39;, &#39;High Staking Participation&#39;]) y = df[&#39;High Staking Participation&#39;] # Handle imbalanced data smote = SMOTE(random_state=42) X_res, y_res = smote.fit_resample(X, y) # Split the data X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42) # Hyperparameter tuning param_grid = { &#39;n_estimators&#39;: [100, 200, 300], &#39;max_depth&#39;: [None, 10, 20, 30], &#39;min_samples_split&#39;: [2, 5, 10], &#39;min_samples_leaf&#39;: [1, 2, 4] } grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=3, n_jobs=1, verbose=2) grid_search.fit(X_train, y_train) best_clf = grid_search.best_estimator_ # Predictions y_pred = best_clf.predict(X_test) # Evaluation accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred) print(f&#39;Best Parameters: {grid_search.best_params_}&#39;) print(f&#39;Accuracy: {accuracy}&#39;) print(f&#39;Classification Report: n{report}&#39;) . Fitting 3 folds for each of 108 candidates, totalling 324 fits [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time= 0.1s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 0.2s [CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time= 0.2s Best Parameters: {&#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;n_estimators&#39;: 200} Accuracy: 0.90625 Classification Report: precision recall f1-score support 0 0.94 0.88 0.91 17 1 0.88 0.93 0.90 15 accuracy 0.91 32 macro avg 0.91 0.91 0.91 32 weighted avg 0.91 0.91 0.91 32 .",
            "url": "https://tunguyenlam.github.io/blog/2024/05/24/Transform-user-into-vector-space.html",
            "relUrl": "/2024/05/24/Transform-user-into-vector-space.html",
            "date": " • May 24, 2024"
        }
        
    
  
    
        ,"post4": {
            "title": "Waiting time for Commuter",
            "content": "When you coming to a train station, you will think about: &quot;How long do I need to wait for comming train?&quot; . In 2022, the average waiting time on a commute trip in Singapore amounted to about nine minutes. The public transport system in Singapore is made up of buses and two different rail systems: the mass rapid transit (MRT) and light rail transit (LRT). . To estimate the waiting time, we need a probabilistic distribution that is able to model happening event during some specific time. Poisson distribution $ mathcal{P}$, is a good way do do that: $$ mathcal{P} = frac{ lambda^k * e^{- lambda}}{k!} $$ . $ lambda$ is a rate: number of events given a interval. eg: 3 calls per minutes at a call center. Then $ mathcal{P}$ is the probability that we have k events in the same interval. in the call center: Receiving k = 1 to 4 calls then has a probability of about 0.77, while receiving 0 or at least 5 calls has a probability of about 0.23. . Time between trains . The waiting time also depends on the time between trains. If the time from previous train and next train is small, customer likely needs to wait during shorted time. Otherwise, if the time from previous train and next train is longer, customers likely needs to wait longer. Somehow, we have a mixture distribution here that contains time between trains. . Assumption is we have data of time between trains. In reality, we are able to extract them from logs, APIs or measure them. Now, we are going to model the distribution of time between trains . Implementation . import matplotlib.pyplot as plt import numpy as np from scipy.stats import gaussian_kde . time_between_trains = [ 428.0, 705.0, 407.0, 465.0, 433.0, 425.0, 204.0, 506.0, 143.0, 351.0, 450.0, 598.0, 464.0, 749.0, 341.0, 586.0, 754.0, 256.0, 378.0, 435.0, 176.0, 405.0, 360.0, 519.0, 648.0, 374.0, 483.0, 537.0, 578.0, 534.0, 577.0, 619.0, 538.0, 331.0, 186.0, 629.0, 193.0, 360.0, 660.0, 484.0, 512.0, 315.0, 457.0, 404.0, 740.0, 388.0, 357.0, 485.0, 567.0, 160.0, 428.0, 387.0, 901.0, 187.0, 622.0, 616.0, 585.0, 474.0, 442.0, 499.0, 437.0, 620.0, 351.0, 286.0, 373.0, 232.0, 393.0, 745.0, 636.0, 758.0, ] . zs = np.array(time_between_trains) / 60 zs.min(), zs.max(), len(zs) . (2.3833333333333333, 15.016666666666667, 70) . qs = np.linspace(0, 20, 101) kde = gaussian_kde(zs) ps = kde(qs) ps = ps/np.sum(ps) len(ps), len(qs) . (101, 101) . plt.plot(qs, ps) plt.grid() plt.show() . likelihood = np.linspace(0, 20, 101) posterior = likelihood * ps posterior = posterior/np.sum(posterior) . plt.plot(qs, ps, label = &quot;prior&quot;) plt.plot(qs, posterior, label = &quot;posterior&quot;) plt.legend() plt.grid() plt.show() . Cummuter comming events . Assume that cummuter comes to station with constant rate $ lambda$, then the probability that there are k commuters come: $$ mathcal{P} = frac{ lambda ^ k cdot e^{- lambda}} {k!} $$ . from scipy.stats import poisson lambdas = [] poisson(0).pmf(0) . 1.0 .",
            "url": "https://tunguyenlam.github.io/blog/2024/04/01/Waiting-time-for-commuter.html",
            "relUrl": "/2024/04/01/Waiting-time-for-commuter.html",
            "date": " • Apr 1, 2024"
        }
        
    
  
    
        ,"post5": {
            "title": "Is cosine similarity good enough",
            "content": "cosine similarity . cosine similarity is the most popular approach to assess similarity between datapoints in Machine Learning. dot(A,B)=∣A∣∣B∣cos(A,B)dot(A,B) = |A||B|cos(A,B)dot(A,B)=∣A∣∣B∣cos(A,B) . The weakness of Cosine Similarity . However, Cosine Similarity cannot measure the attitude of datapoints in space. .",
            "url": "https://tunguyenlam.github.io/blog/2024/03/24/Is-cosine-similarity-good-enough.html",
            "relUrl": "/2024/03/24/Is-cosine-similarity-good-enough.html",
            "date": " • Mar 24, 2024"
        }
        
    
  
    
        ,"post6": {
            "title": "How to encode categorical feature",
            "content": "Abstraction . Encoding is a way we transform data from a representation to another representation. In Machine Learning, we usually use this concept when we want to transform non-numeric data to numeric data. such as, we transform label &quot;cat&quot;, &quot;dog&quot; to 1, 0 respectively. . There are some popular ways to encode categorical feature: Using dictionary to convert data to index, using one-hot vector approach, or we just train a neural network and encode data as word2vec. . Problem of some popular encoding techniques . 1) one-hot: memory . 2) Using dictionary:big dictionary, unknown item . 3) word2vec: unknown item . 4) neural network: need to represent input in some way first, then we need to be back to 1) 2)3). Sometimes, we will get too complex output. eg: encode by GPT. . Addtionally, there are some cases we use sparse matrix to encode.... . import pandas as pd # Sample dataset with a categorical column data = {&#39;Color&#39;: [&#39;Red&#39;, &#39;Blue&#39;, &#39;Green&#39;, &#39;Red&#39;, &#39;Green&#39;]} df = pd.DataFrame(data) # Perform one-hot encoding using Pandas one_hot_encoded = pd.get_dummies(df, columns=[&#39;Color&#39;]).astype(int) one_hot_encoded . Color_Blue Color_Green Color_Red . 0 0 | 0 | 1 | . 1 1 | 0 | 0 | . 2 0 | 1 | 0 | . 3 0 | 0 | 1 | . 4 0 | 1 | 0 | . from sklearn.preprocessing import LabelEncoder data = {&#39;Size&#39;: [&#39;Small&#39;, &#39;Medium&#39;, &#39;Large&#39;, &#39;Medium&#39;, &#39;Small&#39;]} df = pd.DataFrame(data) label_encoder = LabelEncoder() df[&#39;Size_encoded&#39;] = label_encoder.fit_transform(df[&#39;Size&#39;]) df . Size Size_encoded . 0 Small | 2 | . 1 Medium | 1 | . 2 Large | 0 | . 3 Medium | 1 | . 4 Small | 2 | . from numpy import array from scipy.sparse import csr_matrix A = array([[1, 0, 0, 1, 0, 0], [0, 0, 2, 0, 0, 1], [0, 0, 0, 2, 0, 0]]) print(A) print(&quot;&quot;) S = csr_matrix(A) print(S) . [[1 0 0 1 0 0] [0 0 2 0 0 1] [0 0 0 2 0 0]] (0, 0) 1 (0, 3) 1 (1, 2) 2 (1, 5) 1 (2, 3) 2 . Solution: Byte Pair Encoding . Byte Pair Encoding . Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is &quot;byte-level&quot; because it runs on UTF-8 encoded strings. . The tricky thing to note is that minbpe always allocates the 256 individual bytes as tokens, and then merges bytes as needed from there. So for us a=97, b=98, c=99, d=100 (their ASCII values). . References: https://en.wikipedia.org/wiki/Byte_pair_encoding . https://github.com/karpathy/minbpe . import sys from utils import * auto_config() from minbpe.minbpe import BasicTokenizer tokenizer = BasicTokenizer() text = &quot;aaabdaaabac&quot; tokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges print(tokenizer.encode(text)) # [258, 100, 258, 97, 99] print(tokenizer.decode([258, 100, 258, 97, 99])) . [258, 100, 258, 97, 99] aaabdaaabac . unknown_word = &quot;hehe&quot; tokenizer.encode(unknown_word) . [104, 101, 104, 101] .",
            "url": "https://tunguyenlam.github.io/blog/2024/02/29/How-to-encode-categorical-feature.html",
            "relUrl": "/2024/02/29/How-to-encode-categorical-feature.html",
            "date": " • Feb 29, 2024"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is blog where I’m able to write everything about Machine Learning. .",
          "url": "https://tunguyenlam.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About",
          "content": "About this site .",
          "url": "https://tunguyenlam.github.io/blog/about.qmd",
          "relUrl": "/about.qmd",
          "date": ""
      }
      
  

  

  
      ,"page4": {
          "title": "blog",
          "content": "This is a Quarto website. . To learn more about Quarto websites visit https://quarto.org/docs/websites. .",
          "url": "https://tunguyenlam.github.io/blog/index.qmd",
          "relUrl": "/index.qmd",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tunguyenlam.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}