{
  
    
        "post0": {
            "title": "Transform user into vector space",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . Data . We&#39;ll build a small program to generate a sample dataset of user that has schema below: . Wallet Address | Transaction Count | Transaction Volume | Avg Transaction Value | Transaction Frequency | Asset Diversity | Liquidity Provision | Staking Activity | Yield Farming Participation | Borrowing Value | Borrowing Frequency | Lending Value | Lending Frequency | Active Periods| Transaction Recency | Historical Activity Trends . import random import string def random_wallet_address(): return &quot;0x&quot; + &#39;&#39;.join(random.choices(string.ascii_letters + string.digits, k=40)) def random_time_frame(): return random.choice([&#39;Daily&#39;, &#39;Weekly&#39;, &#39;Bi-weekly&#39;, &#39;Monthly&#39;, &#39;Never&#39;]) def random_activity_trend(): return random.choice([&#39;Increasing&#39;, &#39;Stable&#39;, &#39;Decreasing&#39;]) def random_active_periods(): periods = [ &#39;Weekdays 10am-4pm&#39;, &#39;Weekdays 6pm-10pm&#39;, &#39;Weekdays 9am-5pm&#39;, &#39;Weekdays 7am-9am&#39;, &#39;Weekends 1pm-5pm&#39;, &#39;Weekends 8pm-12am&#39;, &#39;Weekends 6pm-10pm&#39;, &#39;Weekdays 11am-3pm&#39;, &#39;Weekends 10am-2pm&#39; ] return random.choice(periods) # Generate 100 records data = [] for _ in range(100): record = { &quot;Wallet Address&quot;: random_wallet_address(), &quot;Transaction Count&quot;: random.randint(10, 200), &quot;Transaction Volume&quot;: random.randint(1000, 50000), &quot;Avg Transaction Value&quot;: random.randint(200, 300), &quot;Transaction Frequency&quot;: random_time_frame(), &quot;Asset Diversity&quot;: random.randint(1, 10), &quot;Liquidity Provision&quot;: random.randint(1000, 20000), &quot;Staking Activity&quot;: random.randint(500, 10000), &quot;Yield Farming Participation&quot;: random.randint(0, 15000), &quot;Borrowing Value&quot;: random.randint(0, 5000), &quot;Borrowing Frequency&quot;: random_time_frame(), &quot;Lending Value&quot;: random.randint(0, 7000), &quot;Lending Frequency&quot;: random_time_frame(), &quot;Active Periods&quot;: random_active_periods(), &quot;Transaction Recency&quot;: random.choice([&quot;1 day ago&quot;, &quot;2 days ago&quot;, &quot;3 days ago&quot;, &quot;1 week ago&quot;, &quot;2 weeks ago&quot;, &quot;1 month ago&quot;]), &quot;Historical Activity Trends&quot;: random_activity_trend() } data.append(record) df = pd.DataFrame(data=data) df.head(10) . Wallet Address Transaction Count Transaction Volume Avg Transaction Value Transaction Frequency Asset Diversity Liquidity Provision Staking Activity Yield Farming Participation Borrowing Value Borrowing Frequency Lending Value Lending Frequency Active Periods Transaction Recency Historical Activity Trends . 0 0xZtSL5C7FLqpL8taTU6PnIo4akNAygDPv8LIve5Eo | 173 | 42997 | 237 | Never | 8 | 13732 | 7750 | 3866 | 926 | Weekly | 265 | Daily | Weekdays 9am-5pm | 1 week ago | Decreasing | . 1 0xZVLjJnulFRiTIJzyqrjZ5ueT7Zcjj837t5hlnGNQ | 104 | 16101 | 241 | Daily | 8 | 5352 | 8296 | 11693 | 3700 | Weekly | 240 | Never | Weekends 10am-2pm | 2 days ago | Increasing | . 2 0xnsDi2oaINtpeh4z0rbUxlVCqknJwAKRmc8YEKrAh | 15 | 35321 | 291 | Never | 3 | 14002 | 6541 | 143 | 305 | Daily | 4785 | Weekly | Weekdays 11am-3pm | 2 weeks ago | Stable | . 3 0xsgTzACi7koBJ2bDM2CdGJndAVPwEZ3tqamAB3VAM | 50 | 20939 | 271 | Bi-weekly | 3 | 9237 | 5118 | 3349 | 3012 | Monthly | 2990 | Daily | Weekdays 6pm-10pm | 1 day ago | Stable | . 4 0xIpK8nQMwu0t9G4nZGRK20vhIvDDnrw8vvhVYOH5U | 79 | 26170 | 238 | Monthly | 3 | 18974 | 5131 | 3088 | 1468 | Weekly | 2665 | Daily | Weekdays 11am-3pm | 1 week ago | Increasing | . 5 0xygIXeoZkbIIiA8QX2YLKc9SQISdElAJoGsTT0mQe | 106 | 45327 | 218 | Weekly | 1 | 2505 | 1502 | 3974 | 4301 | Weekly | 3299 | Never | Weekends 10am-2pm | 1 month ago | Decreasing | . 6 0xOdW6MRLN9b0p3zPwHe4bk8fNjAEimKcKeeQQrsMj | 46 | 30378 | 213 | Bi-weekly | 10 | 13550 | 9656 | 9985 | 1818 | Bi-weekly | 3669 | Monthly | Weekdays 11am-3pm | 3 days ago | Stable | . 7 0x7AR0qrLBkgOyDrt7R0OrSPOAUYbSDKhhiqcGWXw0 | 25 | 29620 | 227 | Never | 3 | 7656 | 4984 | 11914 | 1704 | Monthly | 6868 | Monthly | Weekdays 11am-3pm | 3 days ago | Decreasing | . 8 0xVfi4iAWPK4hRZYRHjWHJWThnK9kolkEBsM6EtIN3 | 30 | 38989 | 271 | Weekly | 8 | 10744 | 1787 | 2728 | 4669 | Never | 6050 | Never | Weekdays 6pm-10pm | 1 day ago | Stable | . 9 0xFwE5rk3erpqgv73gKLZ2n92AJ8Xz2o2iQ4JkWsR7 | 58 | 26675 | 202 | Daily | 1 | 11885 | 7413 | 1346 | 82 | Daily | 2336 | Monthly | Weekends 10am-2pm | 1 month ago | Stable | . Transform user to vector . $$u_i = f(x_i)$$ . $f$:is a transform function. . $x_i$: is a features set of user. . $u_i$: is a vector of specific user. . Similarity . Assume we have a set of products that represented by matrix $P$ where each product is presented by vector $p$. Now, we want to list all of top (eg: 10) product that they are the most similar with product $p_i$. . From matrix $P$, we compute covariance matrix $Q$, that is formed: $$ begin{aligned} Q = P cdot P^T end{aligned} $$ . The, the similar score of $p_i$ will be $q_i$, to get the top similarity of $p_i$: $$top_i = argmax(q_i)$$ . (we also ignore product $i$ in the top_i as itself similar score is 1) . products = np.random.random(size=(10, 10)) norm_products = products/np.linalg.norm(products, axis=1)[:, None] Q = np.dot(norm_products, norm_products.T) sns.heatmap(Q, annot=True, fmt=&#39;.2f&#39;, cmap=&#39;RdYlGn&#39;) . &lt;AxesSubplot:&gt; . Classification users .",
            "url": "https://tunguyenlam.github.io/blog/2024/05/24/Transform-user-into-vector-space.html",
            "relUrl": "/2024/05/24/Transform-user-into-vector-space.html",
            "date": " • May 24, 2024"
        }
        
    
  
    
        ,"post1": {
            "title": "Waiting time for Commuter",
            "content": "When you coming to a train station, you will think about: &quot;How long do I need to wait for comming train?&quot; . In 2022, the average waiting time on a commute trip in Singapore amounted to about nine minutes. The public transport system in Singapore is made up of buses and two different rail systems: the mass rapid transit (MRT) and light rail transit (LRT). . To estimate the waiting time, we need a probabilistic distribution that is able to model happening event during some specific time. Poisson distribution $ mathcal{P}$, is a good way do do that: $$ mathcal{P} = frac{ lambda^k * e^{- lambda}}{k!} $$ . $ lambda$ is a rate: number of events given a interval. eg: 3 calls per minutes at a call center. Then $ mathcal{P}$ is the probability that we have k events in the same interval. in the call center: Receiving k = 1 to 4 calls then has a probability of about 0.77, while receiving 0 or at least 5 calls has a probability of about 0.23. . Time between trains . The waiting time also depends on the time between trains. If the time from previous train and next train is small, customer likely needs to wait during shorted time. Otherwise, if the time from previous train and next train is longer, customers likely needs to wait longer. Somehow, we have a mixture distribution here that contains time between trains. . Assumption is we have data of time between trains. In reality, we are able to extract them from logs, APIs or measure them. Now, we are going to model the distribution of time between trains . Implementation . import matplotlib.pyplot as plt import numpy as np from scipy.stats import gaussian_kde . time_between_trains = [ 428.0, 705.0, 407.0, 465.0, 433.0, 425.0, 204.0, 506.0, 143.0, 351.0, 450.0, 598.0, 464.0, 749.0, 341.0, 586.0, 754.0, 256.0, 378.0, 435.0, 176.0, 405.0, 360.0, 519.0, 648.0, 374.0, 483.0, 537.0, 578.0, 534.0, 577.0, 619.0, 538.0, 331.0, 186.0, 629.0, 193.0, 360.0, 660.0, 484.0, 512.0, 315.0, 457.0, 404.0, 740.0, 388.0, 357.0, 485.0, 567.0, 160.0, 428.0, 387.0, 901.0, 187.0, 622.0, 616.0, 585.0, 474.0, 442.0, 499.0, 437.0, 620.0, 351.0, 286.0, 373.0, 232.0, 393.0, 745.0, 636.0, 758.0, ] . zs = np.array(time_between_trains) / 60 zs.min(), zs.max(), len(zs) . (2.3833333333333333, 15.016666666666667, 70) . qs = np.linspace(0, 20, 101) kde = gaussian_kde(zs) ps = kde(qs) ps = ps/np.sum(ps) len(ps), len(qs) . (101, 101) . plt.plot(qs, ps) plt.grid() plt.show() . likelihood = np.linspace(0, 20, 101) posterior = likelihood * ps posterior = posterior/np.sum(posterior) . plt.plot(qs, ps, label = &quot;prior&quot;) plt.plot(qs, posterior, label = &quot;posterior&quot;) plt.legend() plt.grid() plt.show() . Cummuter comming events . Assume that cummuter comes to station with constant rate $ lambda$, then the probability that there are k commuters come: $$ mathcal{P} = frac{ lambda ^ k cdot e^{- lambda}} {k!} $$ . from scipy.stats import poisson lambdas = [] poisson(0).pmf(0) . 1.0 .",
            "url": "https://tunguyenlam.github.io/blog/2024/04/01/Waiting-time-for-commuter.html",
            "relUrl": "/2024/04/01/Waiting-time-for-commuter.html",
            "date": " • Apr 1, 2024"
        }
        
    
  
    
        ,"post2": {
            "title": "Is cosine similarity good enough",
            "content": "cosine similarity . cosine similarity is the most popular approach to assess similarity between datapoints in Machine Learning. dot(A,B)=∣A∣∣B∣cos(A,B)dot(A,B) = |A||B|cos(A,B)dot(A,B)=∣A∣∣B∣cos(A,B) . The weakness of Cosine Similarity . However, Cosine Similarity cannot measure the attitude of datapoints in space. .",
            "url": "https://tunguyenlam.github.io/blog/2024/03/24/Is-cosine-similarity-good-enough.html",
            "relUrl": "/2024/03/24/Is-cosine-similarity-good-enough.html",
            "date": " • Mar 24, 2024"
        }
        
    
  
    
        ,"post3": {
            "title": "How to encode categorical feature",
            "content": "Abstraction . Encoding is a way we transform data from a representation to another representation. In Machine Learning, we usually use this concept when we want to transform non-numeric data to numeric data. such as, we transform label &quot;cat&quot;, &quot;dog&quot; to 1, 0 respectively. . There are some popular ways to encode categorical feature: Using dictionary to convert data to index, using one-hot vector approach, or we just train a neural network and encode data as word2vec. . Problem of some popular encoding techniques . 1) one-hot: memory . 2) Using dictionary:big dictionary, unknown item . 3) word2vec: unknown item . 4) neural network: need to represent input in some way first, then we need to be back to 1) 2)3). Sometimes, we will get too complex output. eg: encode by GPT. . Addtionally, there are some cases we use sparse matrix to encode.... . import pandas as pd # Sample dataset with a categorical column data = {&#39;Color&#39;: [&#39;Red&#39;, &#39;Blue&#39;, &#39;Green&#39;, &#39;Red&#39;, &#39;Green&#39;]} df = pd.DataFrame(data) # Perform one-hot encoding using Pandas one_hot_encoded = pd.get_dummies(df, columns=[&#39;Color&#39;]).astype(int) one_hot_encoded . Color_Blue Color_Green Color_Red . 0 0 | 0 | 1 | . 1 1 | 0 | 0 | . 2 0 | 1 | 0 | . 3 0 | 0 | 1 | . 4 0 | 1 | 0 | . from sklearn.preprocessing import LabelEncoder data = {&#39;Size&#39;: [&#39;Small&#39;, &#39;Medium&#39;, &#39;Large&#39;, &#39;Medium&#39;, &#39;Small&#39;]} df = pd.DataFrame(data) label_encoder = LabelEncoder() df[&#39;Size_encoded&#39;] = label_encoder.fit_transform(df[&#39;Size&#39;]) df . Size Size_encoded . 0 Small | 2 | . 1 Medium | 1 | . 2 Large | 0 | . 3 Medium | 1 | . 4 Small | 2 | . from numpy import array from scipy.sparse import csr_matrix A = array([[1, 0, 0, 1, 0, 0], [0, 0, 2, 0, 0, 1], [0, 0, 0, 2, 0, 0]]) print(A) print(&quot;&quot;) S = csr_matrix(A) print(S) . [[1 0 0 1 0 0] [0 0 2 0 0 1] [0 0 0 2 0 0]] (0, 0) 1 (0, 3) 1 (1, 2) 2 (1, 5) 1 (2, 3) 2 . Solution: Byte Pair Encoding . Byte Pair Encoding . Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is &quot;byte-level&quot; because it runs on UTF-8 encoded strings. . The tricky thing to note is that minbpe always allocates the 256 individual bytes as tokens, and then merges bytes as needed from there. So for us a=97, b=98, c=99, d=100 (their ASCII values). . References: https://en.wikipedia.org/wiki/Byte_pair_encoding . https://github.com/karpathy/minbpe . import sys from utils import * auto_config() from minbpe.minbpe import BasicTokenizer tokenizer = BasicTokenizer() text = &quot;aaabdaaabac&quot; tokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges print(tokenizer.encode(text)) # [258, 100, 258, 97, 99] print(tokenizer.decode([258, 100, 258, 97, 99])) . [258, 100, 258, 97, 99] aaabdaaabac . unknown_word = &quot;hehe&quot; tokenizer.encode(unknown_word) . [104, 101, 104, 101] .",
            "url": "https://tunguyenlam.github.io/blog/2024/02/29/How-to-encode-categorical-feature.html",
            "relUrl": "/2024/02/29/How-to-encode-categorical-feature.html",
            "date": " • Feb 29, 2024"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is blog where I’m able to write everything about Machine Learning. .",
          "url": "https://tunguyenlam.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About",
          "content": "About this site .",
          "url": "https://tunguyenlam.github.io/blog/about.qmd",
          "relUrl": "/about.qmd",
          "date": ""
      }
      
  

  

  
      ,"page4": {
          "title": "blog",
          "content": "This is a Quarto website. . To learn more about Quarto websites visit https://quarto.org/docs/websites. .",
          "url": "https://tunguyenlam.github.io/blog/index.qmd",
          "relUrl": "/index.qmd",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tunguyenlam.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}