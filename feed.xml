<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://tunguyenlam.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tunguyenlam.github.io/blog/" rel="alternate" type="text/html" /><updated>2024-03-24T02:35:36-05:00</updated><id>https://tunguyenlam.github.io/blog/feed.xml</id><title type="html">Tu’s blog</title><subtitle>blog for Machine Learning.</subtitle><entry><title type="html">Is cosine similarity good enough</title><link href="https://tunguyenlam.github.io/blog/2024/03/24/Is-cosine-similarity-good-enough.html" rel="alternate" type="text/html" title="Is cosine similarity good enough" /><published>2024-03-24T00:00:00-05:00</published><updated>2024-03-24T00:00:00-05:00</updated><id>https://tunguyenlam.github.io/blog/2024/03/24/Is%20cosine%20similarity%20good%20enough</id><author><name></name></author><summary type="html">cosine similarity cosine similarity is the most popular approach to assess similarity between datapoints in Machine Learning. dot(A,B)=∣A∣∣B∣cos(A,B)dot(A,B) = |A||B|cos(A,B)dot(A,B)=∣A∣∣B∣cos(A,B)</summary></entry><entry><title type="html">How to encode categorical feature</title><link href="https://tunguyenlam.github.io/blog/2024/02/29/How-to-encode-categorical-feature.html" rel="alternate" type="text/html" title="How to encode categorical feature" /><published>2024-02-29T00:00:00-06:00</published><updated>2024-02-29T00:00:00-06:00</updated><id>https://tunguyenlam.github.io/blog/2024/02/29/How-to-encode-categorical-feature</id><author><name></name></author><summary type="html"></summary></entry></feed>